{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The goal of this project is to build a Computer Vision Segmentation Model to segment Retinal Blood Vessels.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Installing required ```libraries``` for the project-**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch torchvision torchsummary matplotlib pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing those ```libraries```-**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating a Custom ``RetinalDataset`` Class in ``PyTorch`` for the Training Retinal Images-**\n",
    "1. **Input Shape-** The ```input_shape``` of the retinal images is initially **3⨯512⨯512.**\n",
    "2. **Dataset Filepaths-**\n",
    "    - ``image_ds_path`` stores the path to the retinal images\n",
    "    - ``mask_ds_path`` stores the path to the corresponding retinal masks.\n",
    "3. **Defining the ``RetinalDataset`` Class-**\n",
    "    - Creating a custom class by extending the ``Dataset`` class from ``torch.utils.data``.\n",
    "    - ``__init__`` initializes the dataset's **attributes** such as filepaths and ``transforms`` using specified **parameters**.\n",
    "    - ``__len__`` returns the total number of samples in the ``RetinalDataset`` **(80).**\n",
    "    - ``__getitem__`` retrieves the **grayscale** image and  mask at a specified ``idx``, applies the ``transforms``, and then returns **transformed_image** and **transformed_mask.**\n",
    "4. **Composting the Transformations-**\n",
    "    - ``transforms.Resize()`` ensures the images and masks are their original **W⨯H.**\n",
    "    - ``transforms.ToTensor()`` **normalizes** the pixel values to be **[0,1].**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = torch.tensor([3, 512, 512])\n",
    "\n",
    "image_ds_path = \"C:/Users/User/Retinal_Vessel_Segmentation/train/image/\"\n",
    "mask_ds_path = \"C:/Users/User/Retinal_Vessel_Segmentation/train/mask/\"\n",
    "\n",
    "class RetinalDataset(Dataset):\n",
    "    def __init__(self, image_folder, mask_folder, transforms):\n",
    "        self.image_folder = image_folder\n",
    "        self.mask_folder = mask_folder\n",
    "        self.transforms = transforms\n",
    "        self.retinal_ids = []\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.retinal_ids)\n",
    "\n",
    "    def retinal_id_list(self):\n",
    "        for image in os.listdir(self.image_folder):\n",
    "            self.retinal_ids.append(image)\n",
    "        return self.retinal_ids\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.image_folder, self.retinal_ids[idx])\n",
    "        mask_path = os.path.join(self.mask_folder, self.retinal_ids[idx])\n",
    "\n",
    "        image = Image.open(image_path).convert('L')\n",
    "        mask = Image.open(mask_path).convert('L')\n",
    "\n",
    "        transformed_image = self.transforms(image)\n",
    "        transformed_mask = self.transforms(mask)\n",
    "\n",
    "        return transformed_image, transformed_mask\n",
    "    \n",
    "\n",
    "retinal_transforms = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "RetinalData = RetinalDataset(image_folder=image_ds_path, mask_folder=mask_ds_path, transforms=retinal_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Displaying the first Retinal Image and the respective Blood Vessel Mask using ``matplotlib.pyplot``.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_tensor, mask_tensor = RetinalData.__getitem__(0)\n",
    "image = image_tensor[0].detach().cpu()\n",
    "mask = mask_tensor[0].detach().cpu()\n",
    "\n",
    "print(image_tensor.size())\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.subplot(1, 2, 1) # (1 row, 2 columns, first subplot/column)\n",
    "plt.imshow(image, cmap=\"gray\")\n",
    "plt.title(\"Retina\")\n",
    "\n",
    "plt.subplot(1, 2, 2)  # (1 row, 2 columns, second subplot/column)\n",
    "plt.imshow(mask, cmap=\"gray\")\n",
    "plt.title(\"Retinal Mask\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConvolutional(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.in_c = in_channels\n",
    "        self.out_c = out_channels\n",
    "        self.conv1 = nn.Conv2d(in_channels=self.in_c, out_channels=self.out_c, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=self.out_c, out_channels=self.out_c, kernel_size=3, padding=1)\n",
    "        self.bn = nn.BatchNorm2d(num_features=self.out_c)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        x = self.conv1(input)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.in_c = in_channels\n",
    "        self.out_c = out_channels\n",
    "        self.conv = DoubleConvolutional(in_channels=self.in_c, out_channels=self.out_c)\n",
    "        self.pool = nn.MaxPool2d((2,2))\n",
    "    \n",
    "    def forward(self, input):\n",
    "        x = self.conv(input)\n",
    "        p = self.pool(x)\n",
    "\n",
    "        return x, p\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.in_c = in_channels\n",
    "        self.out_c = out_channels\n",
    "        self.up = nn.ConvTranspose2d(in_channels=self.in_c, out_channels=self.out_c, kernel_size=2, stride=2, padding=0)\n",
    "        self.conv = DoubleConvolutional(in_channels=(self.out_c*2), out_channels=self.out_c)\n",
    "\n",
    "    def forward(self, input, skip):\n",
    "        x = self.up(input)\n",
    "        x = torch.cat([x, skip], axis=1)\n",
    "        x = self.conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNET(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.e1 = Encoder(in_channels=1, out_channels=32)\n",
    "        self.e2 = Encoder(in_channels=32, out_channels=64)\n",
    "        self.e3 = Encoder(in_channels=64, out_channels=128)\n",
    "        self.e4 = Encoder(in_channels=128, out_channels=256)\n",
    "\n",
    "        self.b = DoubleConvolutional(in_channels=256, out_channels=512)\n",
    "\n",
    "        self.d1 = Decoder(in_channels=512, out_channels=256)\n",
    "        self.d2 = Decoder(in_channels=256, out_channels=128)\n",
    "        self.d3 = Decoder(in_channels=128, out_channels=64)\n",
    "        self.d4 = Decoder(in_channels=64, out_channels=32)\n",
    "\n",
    "        self.outputs = nn.Conv2d(in_channels=32, out_channels=1, kernel_size=1, padding=0)\n",
    "\n",
    "    def forward(self, input):\n",
    "        s1, p1 = self.e1(input)\n",
    "        s2, p2 = self.e2(p1)\n",
    "        s3, p3 = self.e3(p2)\n",
    "        s4, p4 = self.e4(p3)\n",
    "\n",
    "        b = self.b(p4)\n",
    "\n",
    "        d1 = self.d1(b, s4)\n",
    "        d2 = self.d2(d1, s3)\n",
    "        d3 = self.d3(d2, s2)\n",
    "        d4 = self.d4(d3, s1)\n",
    "\n",
    "        output = self.outputs(d4)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RetinalSegmentor = UNET()\n",
    "summary(model=RetinalSegmentor, input_size=(1, 512, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image_and_mask(image, true_mask, pred_mask):\n",
    "    \"\"\"Display an image, its ground truth mask, and the predicted mask.\"\"\"\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "    # Show the input image\n",
    "    axs[0].imshow(image.squeeze().cpu().numpy(), cmap='gray')\n",
    "    axs[0].set_title(\"Input Image\")\n",
    "    axs[0].axis(\"off\")\n",
    "\n",
    "    # Show the ground truth mask\n",
    "    axs[1].imshow(true_mask.squeeze().cpu().numpy(), cmap='gray')\n",
    "    axs[1].set_title(\"Ground Truth Mask\")\n",
    "    axs[1].axis(\"off\")\n",
    "\n",
    "    # Show the predicted mask\n",
    "    axs[2].imshow(pred_mask.squeeze().detach().cpu().numpy(), cmap='gray')\n",
    "    axs[2].set_title(\"Generated Mask\")\n",
    "    axs[2].axis(\"off\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "epochs = 10\n",
    "lr = 3e-4\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_loader = DataLoader(dataset=RetinalData, batch_size=batch_size, shuffle=True)\n",
    "r_optimizer = torch.optim.Adam(RetinalSegmentor.parameters(), lr=lr)\n",
    "r_loss_fn = nn.BCEWithLogitsLoss()\n",
    "losses = []\n",
    "\n",
    "def train_model(model, dataloader, optimizer, loss_fn, device, num_epochs, loss_tracker):\n",
    "    model.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}]\")\n",
    "        \n",
    "        for batch_idx, data in enumerate(dataloader):\n",
    "            inputs, masks = data\n",
    "            inputs, masks = inputs.to(device), masks.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, masks)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            print(f\"  Batch [{batch_idx + 1}/{len(dataloader)}]: Loss = {loss.item():.4f}\")\n",
    "        \n",
    "        loss_tracker.append(epoch_loss)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            generated_mask = torch.sigmoid(outputs[0])  # Apply sigmoid for binary segmentation\n",
    "            show_image_and_mask(inputs[0], masks[0], generated_mask)\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}] Average Loss: {epoch_loss / len(dataloader):.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model=RetinalSegmentor, dataloader=train_loader, optimizer=r_optimizer, loss_fn=r_loss_fn, device=device, num_epochs=epochs, loss_tracker=losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, 11)  # 5 epochs\n",
    "\n",
    "# Creating a plot\n",
    "plt.plot(epochs, losses, label='Discriminator Loss', color='#228B22', linewidth=2)\n",
    "\n",
    "# Labeling the axes and the title\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Retinal Blood Vessel Semantic Segmentation Loss')\n",
    "\n",
    "# Show the legend\n",
    "plt.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
